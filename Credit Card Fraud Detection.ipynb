{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Real-life classification problems consist of many cases in which the data is not \"beautifully\" balanced. In such cases, one may find the accuracy score of over 90%, but that may not be the happy ending. This is because we often want to focus only on the minority class, as it plays more important role in such problems. In order to tackle this, we need to come up with proper evaluation metrics and resampling techniques. One of the popular problem regarding imbalanced dataset is Credit Card Fraud Detection. Throughout this problem, we can illustrate well what we need to handle such an imbalanced dataset.\n",
    "\n",
    "## Brief Information about Dataset\n",
    "\n",
    "Class: 0 - non-fraud 1 - fraud\n",
    "Amount: Transaction amount\n",
    "V1,V2,...,V28: These are anonymous features, due to confidentiality. Additionally, these are numerical values which are results of PCA transformation.\n",
    "Time: The amount of seconds elapsed between each transaction and the first transaction in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/yeezhianliew/Downloads/creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Fraud transactions:  0.17\n",
      "Percentage No-fraud transactions:  99.83\n"
     ]
    }
   ],
   "source": [
    "# Check ratio between classes\n",
    "percentage_fraud = round((data['Class'].value_counts()[1] / len (data)) * 100, 2)\n",
    "percentage_no_fraud = round((data['Class'].value_counts()[0] / len(data)) * 100, 2)\n",
    "\n",
    "print ('Percentage Fraud transactions: ', percentage_fraud)\n",
    "print ('Percentage No-fraud transactions: ', percentage_no_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a12f4a5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Class\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud-transactions occupy only 0.17% of the dataset, this dataset is heavily skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...            V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794     ...       0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974     ...      -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643     ...       0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952     ...       0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074     ...       0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  scaled_amount  scaled_time  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0       1.783274    -0.994983  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0      -0.269825    -0.994983  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0       4.983721    -0.994972  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0       1.418291    -0.994972  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0       0.670579    -0.994960  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n",
    "\n",
    "# Get rid of Time and Amount\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "# Let's look at the data again !\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Techniques\n",
    "There are various techniques implemented for dealing with imbalanced dataset. Some popular strategies include improving classification algorithm to fit better with imbalanced dataset, or balancing the classes of training data before providing data as input to the algorithm (data resampling techniques). The second technique is more preferable as it has wider application.\n",
    "\n",
    "Some popular resampling techniques including:\n",
    "\n",
    "1)Random undersampling - UnderSample from imlearn\n",
    "    \n",
    "2)Random oversampling -SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "3)Combine both together\n",
    "\n",
    "\n",
    "## Sampling Explanation \n",
    "Despite the advantage of balancing classes, these techniques also have their weaknesses. The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. Whereas in under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop ('Class', axis = 1)\n",
    "y = data['Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "As the total number of training transactions is too large, which may damage my computer. So I obtain the smaller training dataset with the same ratio of classes of original training dataset. This newly created training dataset is treated as originally skewed training data from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199020\n",
       "1       344\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.concat ([X_train,y_train],axis = 1)\n",
    "training_data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage original fraud:  0.17\n",
      "Percentage original no-fraud:  99.83\n",
      "Number of newly sub fraud data: 170\n",
      "Number of newly sub non-fraud data: 99830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    99830\n",
       "1      170\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Percentage original fraud: ', percentage_fraud)\n",
    "print ('Percentage original no-fraud: ', percentage_no_fraud)\n",
    "number_of_instances = 100000\n",
    "# We will obtain maximum 100.000 data instances with the same class ratio of original data.\n",
    "# Therefore, new data will have 0.17% fraud and 99.83% non-fraud of 100.000.\n",
    "# Which means, new data will have 170 fraud transactions and 99830 non-fraud transactions.\n",
    "\n",
    "number_sub_fraud = int (percentage_fraud/100 * number_of_instances)\n",
    "number_sub_non_fraud = int (percentage_no_fraud/100 * number_of_instances)\n",
    "\n",
    "sub_fraud_data = training_data[training_data['Class'] == 1].head(number_sub_fraud)\n",
    "sub_non_fraud_data = training_data[training_data['Class'] == 0].head(number_sub_non_fraud)\n",
    "\n",
    "print ('Number of newly sub fraud data:',len(sub_fraud_data))\n",
    "print ('Number of newly sub non-fraud data:',len(sub_non_fraud_data))\n",
    "\n",
    "sub_training_data = pd.concat ([sub_fraud_data, sub_non_fraud_data], axis = 0)\n",
    "sub_training_data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    99830\n",
       "1      170\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sub = sub_training_data.drop ('Class', axis = 1)\n",
    "y_train_sub = sub_training_data['Class']\n",
    "y_train_sub.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Under-Sampling the Training Dataset\n",
    "For simplicity, i use DataFrame.sample() to randomly sample the instances of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSample\n",
    "\n",
    "under_algo = RandomUnderSample (random_state = 0)\n",
    "X_train_sub, y_train_sub= under_algo.fit_sample(X_train, y_train)\n",
    "X_train_sub = pd.DataFrame(data = X_train_sub, columns = X_train.columns)\n",
    "y_train_sub = pd.DataFrame(data = y_train_sub, columns = ['Class'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Class\", data=y_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Over-Sampling the Training Dataset\n",
    "I do the same with over-sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a13f79160>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote_algo = SMOTE (random_state = 0)\n",
    "X_train_sub, y_train_sub= smote_algo.fit_sample(X_train, y_train)\n",
    "X_train_sub = pd.DataFrame(data = X_train_sub, columns = X_train.columns)\n",
    "y_train_sub = pd.DataFrame(data = y_train_sub, columns = ['Class'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Class\", data=y_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics in case of Imbalanced Dataset\n",
    "This is a clear example where using a typical accuracy score is no longer appropriate. For example, within this dataset, if we just assign all the class to Non-fraud transactions, we can barely have accuracy score of 99.83% since the original data has 99.83% of non-fraud transactions.\n",
    "On the other hand, we are very interested in the Recall score, because that is the metric that will help us try to capture the most fraudulent transactions.\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    "Recall = TP/(TP+FN)\n",
    "TP: True Positives\n",
    "FP: False Positives\n",
    "FN: False Negatives\n",
    "Where:\n",
    "\n",
    "TP: actually Fraud and predicted as Fraud\n",
    "FP: actually Fraud but predicted as Normal\n",
    "TN: actually Normal and predicted as Normal\n",
    "FN: actually Normal but predicted as Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "svc = SVC()\n",
    "lr = LogisticRegression()\n",
    "svc.fit(X_train_sub, y_train_sub)\n",
    "#Logistic Regression\n",
    "lr.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "# Note: We should test on the original skewed test set\n",
    "predictions_svm = svc.predict(X_test)\n",
    "predictions_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix_svm = confusion_matrix(y_test,predictions_svm)\n",
    "cnf_matrix_lr = confusion_matrix(y_test,predictions_lr)\n",
    "\n",
    "recall_svm = cnf_matrix_svm[1,1]/(cnf_matrix_svm[1,0]+cnf_matrix_svm[1,1])\n",
    "recall_lr = cnf_matrix_lr[1,1]/(cnf_matrix_lr[1,0]+cnf_matrix_lr[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85292     3]\n",
      " [   77    71]]\n"
     ]
    }
   ],
   "source": [
    "print (cnf_matrix_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85281    14]\n",
      " [   54    94]]\n"
     ]
    }
   ],
   "source": [
    "print (cnf_matrix_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4797297297297297 0.6351351351351351\n"
     ]
    }
   ],
   "source": [
    "print (recall_svm,recall_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "After running all the codes above, here are the Recall scores that I obtained:\n",
    "\n",
    "Original Data (Imbalanced)Undersampled DataOversampled DataSVM47.9789.8655.4Logistic Regression63.5189.8689.18\n",
    "\n",
    "For the original skewed dataset, both of the models perform badly. However, with Undersampled data, the Recall scores increase significantly in case of both classifiers. It is noticeable that with Oversampled data, only Logistic Regression can significantly increase the Recall, whereas SVM only slightly enhance it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
